name,quadrant,ring,description
Continuous delivery for machine learning (CD4ML),Techniques,Trial,"<p>Applying machine learning to make the business applications and services intelligent is more than just training models and serving them. It requires implementing end-to-end and continuously repeatable cycles of training, testing, deploying, monitoring and operating the models. <strong><a href=""https://martinfowler.com/articles/cd4ml.html"">Continuous delivery for machine learning (CD4ML)</a></strong> is a technique that enables reliable end-to-end cycles of development, deploying and monitoring machine learning models. The underpinning technology stack to enable CD4ML includes tooling for accessing and discovering data, version control of artefacts (such as data, model and code), continuous delivery pipelines, automated environment provisioning for various deployments and experiments, model performance assessment and tracking, and model operational observability. Companies can choose their own tool set depending on their existing tech stack. CD4ML emphasizes automation and removing manual handoffs. CD4ML is our de facto approach for developing ML models.</p>"
Ethical bias testing,Techniques,Trial,"<p>Over the past year, we've seen a shift in interest around machine learning and deep neural networks in particular. Until now, tool and technique development has been driven by excitement over the remarkable capabilities of these models. Currently, though, there is rising concern that these models could cause unintentional harm. For example, a model could be trained inadvertently to make profitable credit decisions by simply excluding disadvantaged applicants. Fortunately, we're seeing a growing interest in <strong>ethical bias testing</strong> that will help to uncover potentially harmful decisions. Tools such as <a href=""https://github.com/marcotcr/lime"">lime</a>, <a href=""https://aif360.mybluemix.net/"">AI Fairness 360</a> or <a href=""/radar/tools/what-if-tool"">What-If Tool</a> can help uncover inaccuracies that result from underrepresented groups in training data and visualization tools such as <a href=""https://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html"">Google Facets</a> or <a href=""https://pair-code.github.io/facets/"">Facets Dive</a> can be used to discover subgroups within a corpus of training data. We've used lime (local interpretable model-agnostic explanations) in addition to this technique in order to understand the predictions of any machine-learning classifier and what classifiers (or models) are doing.</p>"
Semi-supervised learning loops,Techniques,Trial,"<p><strong>Semi-supervised learning loops</strong> are a class of iterative machine-learning workflows that take advantage of the relationships to be found in unlabeled data. These techniques may improve models by combining labeled and unlabeled data sets in various ways. In other cases they compare models trained on different subsets of the data. Unlike either unsupervised learning where a machine infers classes in unlabeled data or supervised techniques where the training set is entirely labeled, semi-supervised techniques take advantage of a small set of labeled data and a much larger set of unlabeled data. Semi-supervised learning is also closely related to active learning techniques where a human is directed to selectively label ambiguous data points. Since expert humans that can accurately label data are a scarce resource and labeling is often the most time-consuming activity in the machine-learning workflow, semi-supervised techniques lower the cost of training and make machine learning feasible for a new class of users. We're also seeing the application of weakly supervised techniques where machine-labeled data is used but is trusted less than the data labeled by humans.</p>"
Zero trust architecture (ZTA),Techniques,Trial,"<p>The technology landscape of organizations today is increasingly more complex with assets — data, functions, infrastructure and users — spread across security boundaries, such as local hosts, multiple cloud providers and a variety of SaaS vendors. This demands a paradigm shift in enterprise security planning and systems architecture, moving from static and slow-changing security policy management, based on trust zones and network configurations, to dynamic, fine-grained security policy enforcement based on temporal access privileges.</p>

<p><strong>Zero trust architecture (ZTA)</strong> is an organization's strategy and journey to implement zero-trust security principles for all of their assets — such as devices, infrastructure, services, data and users — and includes implementing practices such as securing all access and communications regardless of the network location, enforcing policies as code based on the least privilege and as granular as possible, and continuous monitoring and automated mitigation of threats. Our Radar reflects many of the enabling techniques such as <a href=""/radar/techniques/security-policy-as-code"">security policy as code</a>, <a href=""/radar/techniques/sidecars-for-endpoint-security"">sidecars for endpoint security</a> and <a href=""/radar/techniques/beyondcorp"">BeyondCorp</a>. If you're on your journey toward ZTA, refer to the <a href=""https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-207-draft2.pdf"">NIST ZTA publication</a> to learn more about principles, enabling technology components and migration patterns as well as Google's publication on <a href=""https://cloud.google.com/security/beyondprod"">BeyondProd</a>.</p>"
Data mesh,Techniques,Assess,"<p><strong><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"">Data mesh</a></strong> is an architectural and organizational paradigm that challenges the age-old assumption that we must centralize big analytical data to use it, have data all in one place or be managed by a centralized data team to deliver value. Data mesh claims that for big data to fuel innovation, its ownership must be federated among domain data owners who are accountable for providing their data as products (with the support of a self-serve data platform to abstract the technical complexity involved in serving data products);  it must also adopt a new form of federated governance through automation to enable interoperability of domain-oriented data products. Decentralization, along with interoperability and focus on the experience of data consumers, are key to the democratization of innovation using data.</p>

<p>If your organization has a large number of domains with numerous systems and teams generating data or a diverse set of data-driven use cases and access patterns, we suggest you assess data mesh. Implementation of data mesh requires investment in building a self-serve data platform and embracing an organizational change for domains to take on the long-term ownership of their data products, as well as an incentive structure that rewards domains serving and utilizing data as a product.</p>"
Declarative data pipeline definition,Techniques,Assess,"<p>Many data pipelines are defined in a large, more or less imperative script written in Python or Scala. The script contains the logic of the individual steps as well as the code chaining the steps together. When faced with a similar situation in Selenium tests, developers discovered the Page Object pattern, and later many behavior-driven development (BDD) frameworks implemented a split between step definitions and their composition. Some teams are now experimenting with bringing the same thinking to data engineering. A separate <strong>declarative data pipeline definition</strong>, maybe written in YAML, contains only the declaration and sequence of steps. It states input and output data sets but refers to scripts if and when more complex logic is needed. With <a href=""https://github.com/binaryaffairs/a-la-mode"">A La Mode</a>, we're seeing the first open source tool appear in this space.</p>"
DeepWalk,Techniques,Assess,"<p><strong><a href=""https://github.com/phanein/deepwalk"">DeepWalk</a></strong> is an algorithm that helps apply machine learning on graphs. When working on data sets that are represented as graphs, one of the key problems is to extract features from the graph. This is where DeepWalk can help. It uses SkipGram to construct node embeddings by viewing the graph as a language where each node is a unique word in the language and random walks of finite length on the graph constitutes a sentence. These embeddings can then be used by various ML models. DeepWalk is one of the techniques we're trialling on some of our projects where we've needed to apply machine learning on graphs.</p>"
Google BigQuery ML,Platforms,Assess,"<p>Often training and predicting outcomes from machine learning models require code to take the data to the model. <strong><a href=""https://cloud.google.com/bigquery-ml/docs"">Google BigQuery ML</a></strong> inverts this by bringing the model to the data. <a href=""https://cloud.google.com/bigquery"">Google BigQuery</a> is a data warehouse designed to serve large-scale queries using SQL, for analytical use cases. Google BigQuery ML extends this function and its SQL interface to create, train and evaluate machine learning models using its data sets; and eventually run model predictions to create new BigQuery data sets. It supports a limited set of models out of the box, such as linear regression for forecasting or binary and multiclass regression for classification. It also supports, with limited functionality, importing previously trained <a href=""/radar/languages-and-frameworks/tensorflow"">TensorFlow</a> models. Although BigQuery ML and its SQL-based approach lower the bar for using machine learning to make predictions and recommendations, particularly for quick explorations, this comes with a difficult trade-off: compromising on other aspects of model training such as <a href=""/radar/techniques/ethical-bias-testing"">ethical bias testing</a>, <a href=""/radar/techniques/explainability-as-a-first-class-model-selection-criterion"">explainability</a> and <a href=""/radar/techniques/continuous-delivery-for-machine-learning-cd4ml"">continuous delivery for machine learning</a>.</p>"
JupyterLab,Platforms,Assess,"<p><strong><a href=""https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html"">JupyterLab</a></strong> is the next-generation web-based user interface for Project <a href=""/radar/tools/jupyter"">Jupyter</a>. If you've been using Jupyter Notebooks, JupyterLab is worth a try; it gives you an interactive environment for Jupyter notebooks, code and data. We see it as an evolution of Jupyter Notebook: it provides a better experience by extending its original capabilities of allowing code, visualization and documentation to exist in one place.</p>"
Marquez,Platforms,Assess,"<p><strong><a href=""https://marquezproject.github.io/marquez/"">Marquez</a></strong> is a relatively young open source project for collecting and serving metadata information about a data ecosystem. It represents a simple data model to capture metadata such as lineage, upstream and downstream data processing jobs and their status, and a flexible set of tags to capture the attributes of data sets. It provides a simple <a href=""https://marquezproject.github.io/marquez/openapi.html#"">RESTful API</a> to manage the metadata which eases the integration of Marquez to other tool sets within the data ecosystem.</p>

<p>We've used Marquez as a starting point and easily extended it to fit our needs such as enforcing security policies as well as changes to its domain language. If you're looking for a small and simple tool to bootstrap storage and visualization of your data-processing jobs and data sets, Marquez is a good place to start.</p>"
DVC,Tools,Trial,"<p>In 2018 we mentioned <strong><a href=""https://dvc.org/"">DVC</a></strong> in conjunction with the <a href=""/radar/techniques/versioning-data-for-reproducible-analytics"">versioning data for reproducible analytics</a>. Since then it has become a favorite tool for managing experiments in machine learning (ML) projects. Since it's based on Git, DVC is a familiar environment for software developers to bring their engineering practices to ML practice. Because it versions the code that processes data along with the data itself and tracks stages in a pipeline, it helps bring order to the modeling activities without interrupting the analysts’ flow.</p>"
Experiment tracking tools for machine learning,Tools,Trial,"<p>The day-to-day work of machine learning often boils down to a series of experiments in selecting a modeling approach and the network topology, training data and optimizing or tweaking the model. Data scientists must use experience and intuition to hypothesize changes and then measure the impact those changes have on the overall performance of the model. As this practice has matured, our teams have found an increasing need for <strong>experiment tracking tools for machine learning</strong>. These tools help investigators keep track of the experiments and work through them methodically. Although no clear winner has emerged, tools such as <a href=""https://mlflow.org/"">MLflow</a> and platforms such as <a href=""https://comet.ml"">Comet</a> or <a href=""https://neptune.ml"">Neptune</a> have introduced rigor and repeatability into the entire machine learning workflow.</p>"
Apache Superset,Tools,Assess,"<p><strong><a href=""https://superset.apache.org/"">Apache Superset</a></strong> is a great business intelligence (BI) tool for data exploration and visualization to work with large data lake and data warehouse setups. It works, for example, with <a href=""/radar/platforms/presto"">Presto</a>, <a href=""https://aws.amazon.com/athena/"">Amazon Athena</a> and <a href=""https://aws.amazon.com/redshift/"">Amazon Redshift</a> and can be nicely integrated with enterprise authentication. Moreover, you don't have to be a data engineer to use it; it’s meant to benefit all engineers exploring data in their everyday work. It's worth pointing out that Apache Superset is currently undergoing incubation at the Apache Software Foundation (ASF), meaning it's not yet fully endorsed by ASF.</p>"
Manifold,Tools,Assess,"<p><strong><a href=""https://github.com/uber/manifold"">Manifold</a></strong> is a model-agnostic visual debugger for machine learning (ML). Model developers spend a significant amount of time on iterating and improving an existing model rather than creating a new one. By shifting the focus from model space to data space, Manifold supplements the existing performance metrics with a visual characteristics of the data set that influences the model performance. We think Manifold will be a useful tool to assess in the ML ecosystem.</p>"
PyTorch,languages-and-frameworks,Trial,"<p>Our teams have continued to use and appreciate the <strong><a href=""http://pytorch.org/"">PyTorch</a></strong> machine learning framework, and several teams prefer PyTorch over <a href=""/radar/languages-and-frameworks/tensorflow"">TensorFlow</a>. PyTorch exposes the inner workings of ML that TensorFlow hides, making it easier to debug, and contains constructs that programmers are familiar with such as loops and actions. Recent releases have improved performance of PyTorch, and we've been using it successfully in production projects.</p>"
Deequ,languages-and-frameworks,Assess,"<p>There are still some tool gaps when applying good software engineering practices in data engineering. Attempting to automate data quality checks between different steps in a data pipeline, one of our teams was surprised when they found only a few tools in this space. They settled on <strong><a href=""https://github.com/awslabs/deequ"">Deequ</a></strong>, a library for writing tests that resemble unit tests for data sets. Deequ is built on top of <a href=""/radar/platforms/apache-spark"">Apache Spark</a>, and even though it's published by AWS Labs it can be used in environments other than <a href=""/radar/platforms/aws"">AWS</a>.</p>"